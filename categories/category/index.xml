<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>category on Rui Ren</title>
    <link>https://victormaggie.github.io/categories/category/</link>
    <description>Recent content in category on Rui Ren</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Aug 2020 18:50:57 -0500</lastBuildDate>
    
	<atom:link href="https://victormaggie.github.io/categories/category/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Random Forest</title>
      <link>https://victormaggie.github.io/2020/08/random-forest/</link>
      <pubDate>Sun, 02 Aug 2020 18:50:57 -0500</pubDate>
      
      <guid>https://victormaggie.github.io/2020/08/random-forest/</guid>
      <description>&lt;h2 id=&#34;bagging-and-random-tree&#34;&gt;Bagging and Random Tree&lt;/h2&gt;
&lt;p&gt;  In this project, we delve into the Tree-based machine learning model for Finance. Currently, XGBoost or LightBoost, etc. works well for machine learning projects. However, it is more difficult to built them from scracth because of their optimization for memory or parallel/GPU computing. We then start to build Random Forest and Decision Tree from Scracth. Then discuss how leaf size and prune influence on the performance.&lt;/p&gt;
&lt;h4 id=&#34;1-overfitting-versue-leaf-size&#34;&gt;1. Overfitting versue leaf size&lt;/h4&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt; Overfitting does happen in the decision tree learning module. As we know, the overfitting definition is that the training error is small while the testing error is larger, this means the generalization of this algorithm is poor, and cannot be used for future prediction. As shown in Fig 1, when leaf value is less than 10, that will have overfitting.&lt;/p&gt;
&lt;h4 id=&#34;2-bagging&#34;&gt;2. Bagging&lt;/h4&gt;
&lt;p&gt;  We built a &lt;strong&gt;Random Forest&lt;/strong&gt; tree by bootstrap aggreating method. In order to discuss the how bootstrap can help solve the overfitting, we choose the bag size as 20, and leaf size differs from 1 ~ 18 to do the analysis.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;  As shown in Fig 2, Bagging can reduce the overfitting, comparing to Fig.1, when leaf size is less than 5, the testing RMSE for BagLearner is less than 0.005. However, the testing RMSE is over 0.006 for Decision Tree as shown in Fig. 1.&lt;/p&gt;
&lt;p&gt;  With the increasing of leaf size, the training RSME is increasing rapidly, however, the testing RMSE is very stable around 0.005~0.0045. As such, Bagging can help reduce overfitting and increase the prediction accuracy, as algorithm has characteristics of randomness.&lt;/p&gt;
&lt;h4 id=&#34;3-quantitatively-analysis&#34;&gt;3. Quantitatively analysis&lt;/h4&gt;
&lt;p&gt;  In this chapter, we quantitatively analysis the performance of &lt;code&gt;Random Tree Learning&lt;/code&gt; and &lt;code&gt;Decision Tree Learning&lt;/code&gt;. From Fig. 3 and Fig. 4, we can see that Random Tree can not defeat Decision Tree regarding RMSE. Both algorithm will suffer from overfitting, however, random tree algorithm can reduce the overfitting somehow.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;  Howver, in terms of time complexity, random tree is very fast. As Figure 4, decision tree is linear time complexity. In terms of random tree, it is more like a constant time complexity.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    
  </channel>
</rss>